{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from httpx import AsyncClient, Response\n",
    "from parsel import Selector\n",
    "from loguru import logger as log\n",
    "\n",
    "# initialize an async httpx client\n",
    "client = AsyncClient(\n",
    "    # enable http2\n",
    "    http2=True,\n",
    "    # add basic browser like headers to prevent being blocked\n",
    "    headers={\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    },\n",
    ")\n",
    "\n",
    "def parse_profile(response: Response):\n",
    "    \"\"\"parse profile data from hidden scripts on the HTML\"\"\"\n",
    "    assert response.status_code == 200, \"request is blocked, use the ScrapFly codetabs\"\n",
    "    selector = Selector(response.text)\n",
    "    data = selector.xpath(\"//script[@id='__UNIVERSAL_DATA_FOR_REHYDRATION__']/text()\").get()\n",
    "    profile_data = json.loads(data)[\"__DEFAULT_SCOPE__\"][\"webapp.user-detail\"][\"userInfo\"]\n",
    "    return profile_data\n",
    "    \n",
    "\n",
    "\n",
    "async def scrape_profiles(urls: List[str]) -> List[Dict]:\n",
    "    \"\"\"scrape tiktok profiles data from their URLs\"\"\"\n",
    "    to_scrape = [client.get(url) for url in urls]\n",
    "    data = []\n",
    "    # scrape the URLs concurrently\n",
    "    for response in asyncio.as_completed(to_scrape):\n",
    "        response = await response\n",
    "        profile_data = parse_profile(response)\n",
    "        data.append(profile_data)\n",
    "    log.success(f\"scraped {len(data)} profiles from profile pages\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run():\n",
    "    profile_data = await scrape_profiles(\n",
    "        urls=[\n",
    "            \"https://www.tiktok.com/@oddanimalspecimens\"\n",
    "        ]\n",
    "    )\n",
    "    # save the result to a JSON file\n",
    "    with open(\"profile_data.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(profile_data, file, indent=2, ensure_ascii=False)\n",
    "\n",
    "await run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scrapfly-sdk\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jmespath\n",
    "import asyncio\n",
    "import json\n",
    "from typing import Dict, List\n",
    "from loguru import logger as log\n",
    "from scrapfly import ScrapeConfig, ScrapflyClient, ScrapeApiResponse\n",
    "\n",
    "SCRAPFLY = ScrapflyClient(key=\"scp-live-a2884ac67cc14f75ae06390d067aae3a\")\n",
    "\n",
    "js_scroll_function = \"\"\"\n",
    "function scrollToEnd(i) {\n",
    "    // check if already at the bottom and stop if there aren't more scrolls\n",
    "    if (window.innerHeight + window.scrollY >= document.body.scrollHeight) {\n",
    "        console.log(\"Reached the bottom.\");\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    // scroll down\n",
    "    window.scrollTo(0, document.body.scrollHeight);\n",
    "\n",
    "    // set a maximum of 15 iterations\n",
    "    if (i < 15) {\n",
    "        setTimeout(() => scrollToEnd(i + 1), 3000);\n",
    "    } else {\n",
    "        console.log(\"Reached the end of iterations.\");\n",
    "    }\n",
    "}\n",
    "\n",
    "scrollToEnd(0);\n",
    "\"\"\"\n",
    "\n",
    "def parse_channel(response: ScrapeApiResponse):\n",
    "    \"\"\"parse channel video data from XHR calls\"\"\"\n",
    "    # extract the xhr calls and extract the ones for videos\n",
    "    _xhr_calls = response.scrape_result[\"browser_data\"][\"xhr_call\"]\n",
    "    post_calls = [c for c in _xhr_calls if \"/api/post/item_list/\" in c[\"url\"]]\n",
    "    post_data = []\n",
    "    for post_call in post_calls:\n",
    "        try:\n",
    "            data = json.loads(post_call[\"response\"][\"body\"])[\"itemList\"]\n",
    "        except Exception:\n",
    "            raise Exception(\"Post data couldn't load\")\n",
    "        post_data.extend(data)\n",
    "    # parse all the data using jmespath\n",
    "    parsed_data = []\n",
    "    for post in post_data:\n",
    "        result = jmespath.search(\n",
    "            \"\"\"{\n",
    "            createTime: createTime,\n",
    "            desc: desc,\n",
    "            id: id,\n",
    "            stats: stats,\n",
    "            contents: contents[].{desc: desc, textExtra: textExtra[].{hashtagName: hashtagName}},\n",
    "            video: video\n",
    "            }\"\"\",\n",
    "            post\n",
    "        )\n",
    "        parsed_data.append(result)    \n",
    "    return parsed_data\n",
    "\n",
    "\n",
    "async def scrape_channel(url: str) -> List[Dict]:\n",
    "    \"\"\"scrape video data from a channel (profile with videos)\"\"\"\n",
    "    log.info(f\"scraping channel page with the URL {url} for post data\")\n",
    "    response = await SCRAPFLY.async_scrape(\n",
    "        ScrapeConfig(\n",
    "            url,\n",
    "            asp=True,\n",
    "            country=\"AU\",\n",
    "            render_js=True,\n",
    "            rendering_wait=5000,\n",
    "            js=js_scroll_function,\n",
    "            wait_for_selector=\"//div[@id='main-content-video_detail']\",\n",
    "        )\n",
    "    )\n",
    "    data = parse_channel(response)\n",
    "    log.success(f\"scraped {len(data)} posts data\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run():\n",
    "    channel_data = await scrape_channel(\n",
    "        url=\"https://www.tiktok.com/@alice_weidel_afd\"\n",
    "    )\n",
    "    # save the result to a JSON file\n",
    "    with open(\"channel_data.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(channel_data, file, indent=2, ensure_ascii=False)\n",
    "\n",
    "await run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from scrapfly import ScrapeConfig, ScrapflyClient, ScrapeApiResponse\n",
    "\n",
    "# Initialize Scrapfly with your API key\n",
    "scrapfly = ScrapflyClient(key=\"scp-live-a2884ac67cc14f75ae06390d067aae3a\")\n",
    "\n",
    "\n",
    "# Define the TikTok video URL\n",
    "tiktok_url = \"https://www.tiktok.com/@alice_weidel_afd/video/7472776863199350038\"\n",
    "\n",
    "# Scrape the TikTok page\n",
    "response = scrapfly.scrape(ScrapeConfig(url=tiktok_url, render_js=True))\n",
    "\n",
    "# Extract the video URL (TikTok embeds video URLs in JSON)\n",
    "video_url = response.content.split('\"playAddr\":\"')[1].split('\"')[0].replace(\"\\\\u0026\", \"&\")\n",
    "\n",
    "print(\"Video URL:\", video_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "\n",
    "video_response = httpx.get(\"\")\n",
    "\n",
    "with open(\"tiktok_video_02.mp4\", \"wb\") as f:\n",
    "    f.write(video_response.content)\n",
    "\n",
    "print(\"Video downloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "import re\n",
    "\n",
    "# Replace with the TikTok video URL you want to scrape\n",
    "tiktok_url = \"https://www.tiktok.com/@alice_weidel_afd/video/7472776863199350038\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "response = httpx.get(tiktok_url, headers=headers)\n",
    "\n",
    "# Extract the video URL using regex\n",
    "match = re.search(r'\"playAddr\":\"(https://.+?)\"', response.text)\n",
    "if match:\n",
    "    video_url = match.group(1).replace(\"\\\\u0026\", \"&\")\n",
    "    print(\"Video URL:\", video_url)\n",
    "else:\n",
    "    print(\"Failed to find the video URL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapfly import ScrapeConfig, ScrapflyClient, ScrapeApiResponse\n",
    "\n",
    "scrapfly = ScrapflyClient(key=\"scp-live-a2884ac67cc14f75ae06390d067aae3a\")\n",
    "\n",
    "tiktok_url = \"https://www.tiktok.com/@alice_weidel_afd/video/7472776863199350038\"\n",
    "\n",
    "# Enable JavaScript rendering to fully load the TikTok page\n",
    "response = scrapfly.scrape(ScrapeConfig(url=tiktok_url, render_js=True))\n",
    "\n",
    "# Extract the video URL\n",
    "match = re.search(r'\"playAddr\":\"(https://.+?)\"', response.content)\n",
    "if match:\n",
    "    video_url = match.group(1).replace(\"\\\\u0026\", \"&\")\n",
    "    print(\"Video URL:\", video_url)\n",
    "else:\n",
    "    print(\"Failed to find the video URL.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "#â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“\n",
    "# setup\n",
    "#â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“\n",
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.chromium.launch(headless = False)\n",
    "page = await browser.new_page()\n",
    "search = 'https://www.tiktok.com/@alice_weidel_afd/video/7142743647753096454?lang=en'\n",
    "await page.goto(search)\n",
    "await page.wait_for_load_state(\"domcontentloaded\")\n",
    "#â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“\n",
    "# wait for the needed element to load to ensure it exists \n",
    "#â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“\n",
    "wait = await page.wait_for_selector('.e11s2kul3 .css-xhj3u2-StyledVideoBlurBackground')\n",
    "\n",
    "element = await page.query_selector(\".xgplayer-container\")\n",
    "print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcript and traslate \n",
    "\n",
    "# ðŸ”‘ Set Your DeepL API Key Here\n",
    "DEEPL_API_KEY = \"\"  # Replace with your actual key\n",
    "\n",
    "# Define DeepL Translation Function\n",
    "def translate_text(text, source_lang=\"DE\", target_lang=\"EN\"):\n",
    "    url = \"https://api-free.deepl.com/v2/translate\"  # Use \"api.deepl.com\" for Pro accounts\n",
    "    params = {\n",
    "        \"auth_key\": DEEPL_API_KEY,\n",
    "        \"text\": text,\n",
    "        \"source_lang\": source_lang,\n",
    "        \"target_lang\": target_lang\n",
    "    }\n",
    "    response = requests.post(url, data=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"translations\"][0][\"text\"]\n",
    "    else:\n",
    "        print(\"Translation Error:\", response.text)\n",
    "        return text  # Return original text in case of failure\n",
    "\n",
    "# Load Whisper Model\n",
    "model = WhisperModel(\"large-v3\")  # Use \"small\", \"large-v3\", etc., depending on need\n",
    "\n",
    "# Transcribe the Audio\n",
    "segments, _ = model.transcribe(my_audio, word_timestamps=True)\n",
    "\n",
    "# Define Output CSV File\n",
    "csv_filename = \"/content/drive/MyDrive/Thesis/Code/Translated.csv\"\n",
    "\n",
    "# Open CSV File for Writing\n",
    "with open(csv_filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write Header\n",
    "    writer.writerow([\"Segment Start\", \"Segment End\", \"German Text\", \"English Translation\"])\n",
    "\n",
    "    # Process Each Transcribed Segment\n",
    "    for segment in segments:\n",
    "        english_translation = translate_text(segment.text)\n",
    "        writer.writerow([segment.start, segment.end, segment.text, english_translation])\n",
    "\n",
    "print(f\"Transcription and translation saved to {csv_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
