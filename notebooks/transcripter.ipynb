{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanscripting Video Files From TikTok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing dependencies\n",
    "We need moviepy to convert the videos to mp3\n",
    "We need openai-whisper to tanscript the files to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install moviepy\n",
    "%pip install -U openai-whisper\n",
    "%pip install faster-whisper \n",
    "%pip install pandas \n",
    "%pip install requests\n",
    "%pip install moviepy\n",
    "%pip install -U spacy\n",
    "%pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your notebook to the right directrory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%cd '/Users/noursafadi/Documents/Uni/Parsons-Spring-25/Major Studio 02/Thesis/tiktok-scraper'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import *\n",
    "import whisper\n",
    "import csv\n",
    "import requests\n",
    "from faster_whisper import WhisperModel\n",
    "import httpx\n",
    "import json\n",
    "from os import listdir, path\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading video files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tiktok_vids.json\", \"r\") as file: \n",
    "    data = json.load(file)\n",
    "\n",
    "links = []\n",
    "for idx,post in enumerate(data):\n",
    "    if \"videoMeta\" in post and \"downloadAddr\" in post[\"videoMeta\"]:\n",
    "        url = post[\"videoMeta\"][\"downloadAddr\"]\n",
    "        if url: \n",
    "            video_response = httpx.get(url)\n",
    "            file_path = f\"/videos/{idx}.mp4\"\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(video_response.content)\n",
    "            print(f\"{idx}: Seccessful\")\n",
    "        else:\n",
    "            print(f\"{idx} is not Available\")\n",
    "    else:\n",
    "        print(f\"{idx} is Not Available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting from video to audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/videos\"\n",
    "audio_output_path = \"/audio\"\n",
    "\n",
    "files = [f for f in listdir(video_path) if f.endswith(\".mp4\")]\n",
    "\n",
    "for fname in files:\n",
    "    video_location = path.join(video_path, fname)\n",
    "\n",
    "    try: \n",
    "        video = VideoFileClip(video_location)\n",
    "        audio = video.audio\n",
    "\n",
    "        audio_file_name = path.splitext(fname)[0] + \".mp3\"\n",
    "        audio_file_path = path.join(audio_output_path, audio_file_name)\n",
    "        audio.write_audiofile(audio_file_path, codec=\"mp3\")\n",
    "\n",
    "        video.close()\n",
    "    except Exception as e: \n",
    "        print(f\"Error Processing {fname} : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lodaing Whisper object Model from Open AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OpenAI WHISPER basic transcripting\n",
    "model = whisper.load_model(\"turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transcripting Audio Files with Whisper AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_folder = \"audio/\"\n",
    "audio_files = [f for f in listdir(audio_folder) if f.endswith(\".mp3\")]\n",
    "\n",
    "scripts = []\n",
    "for fname in audio_files:\n",
    "    file = path.join(audio_folder, fname)\n",
    "    result = model.transcribe(file)\n",
    "    scripts.append(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a JSON file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note when using json with a different language, letters such as ä, ö or simillar will be encoded so we will need to deal with that. \n",
    "# That's why I will create a csv file, which keep the letters as they are without any change. \n",
    "json_sting = json.dumps(scripts)\n",
    "json_path = \"data/scripts.json\"\n",
    "with open(json_path, \"w\") as file: \n",
    "    json.dump(scripts,file,indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"data/scripts.csv\"\n",
    "\n",
    "# Open CSV File for Writing\n",
    "with open(csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write Header\n",
    "    writer.writerow([\"id\", \"script\"])\n",
    "\n",
    "    # Process Each Transcribed Segment\n",
    "    for idx, text in enumerate(scripts):\n",
    "        writer.writerow([idx, text])\n",
    "\n",
    "print(f\"Transcription and translation saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading csv as data frame. \n",
    "df = pd.read_csv(\"data/scripts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translating the texts with DEEPL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEEPL API KEY\n",
    "DEEPL_API_KEY = \"39f5e8f7-4926-44de-b567-420c316bc88e:fx\"  # Replace with your actual key\n",
    "\n",
    "# Define DeepL Translation Function\n",
    "def translate_text(text, source_lang=\"DE\", target_lang=\"EN\"):\n",
    "    url = \"https://api-free.deepl.com/v2/translate\"  # Use \"api.deepl.com\" for Pro accounts\n",
    "    params = {\n",
    "        \"auth_key\": DEEPL_API_KEY,\n",
    "        \"text\": text,\n",
    "        \"source_lang\": source_lang,\n",
    "        \"target_lang\": target_lang\n",
    "    }\n",
    "    response = requests.post(url, data=params)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"translations\"][0][\"text\"]\n",
    "    else:\n",
    "        print(\"Translation Error:\", response.text)\n",
    "        return text  # Return original text in case of failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a loop to translate and store the translated text in a list\n",
    "translated_list = []\n",
    "for index, row in df.iterrows():\n",
    "    #print(f\"{row['script']}\")\n",
    "    translated_list.append(translate_text(row['script']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the translated scripts as csv file with index's \n",
    "csv_path = \"data/translated_scripts.csv\"\n",
    "\n",
    "# Open CSV File for Writing\n",
    "with open(csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write Header\n",
    "    writer.writerow([\"id\", \"script\"])\n",
    "\n",
    "    for idx, text in enumerate(translated_list):\n",
    "        writer.writerow([idx, text])\n",
    "    \n",
    "print(f\"Transcription and translation saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy Framework for efficient categories and keywords handeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_translated = pd.read_csv(\"data/translated_scripts.csv\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for index, row in df_translated.iterrows():\n",
    "    print(index)\n",
    "    #print(row[\"script\"])\n",
    "    text = row[\"script\"]\n",
    "    doc = nlp(text)\n",
    "    for entity in doc.ents:\n",
    "            print(\"This is an Entity\",entity.text, entity.label_)\n",
    "    print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "    print(\"Verbs: \", [token.lemma_ for token in doc if token.pos_ ==\"VERB\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the rows that did not get translated in order to apply DEEPL function again if not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index with Germany Text: [36, 87, 91, 99, 101, 109, 113, 135, 136, 139, 141, 144, 148, 149, 150, 159, 160, 163]\n"
     ]
    }
   ],
   "source": [
    "df_translated = pd.read_csv(\"data/translated_scripts.csv\")\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "\n",
    "#detecting each row\n",
    "df_translated[\"language\"] = df_translated[\"script\"].astype(str).apply(detect_language)\n",
    "\n",
    "german_index = df_translated[df_translated[\"language\"] == \"de\"].index.tolist()\n",
    "\n",
    "print(f\"index with Germany Text: {german_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_list_second = []\n",
    "\n",
    "for index, row in df_translated.iterrows():\n",
    "    if row[\"language\"] == \"de\":\n",
    "        translated = translate_text(row[\"script\"])\n",
    "        translated_list_second.append(translated)\n",
    "    else: \n",
    "        translated_list_second.append(row[\"script\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription and translation saved to data/translated_scripts_checked.csv\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"data/translated_scripts_checked.csv\"\n",
    "\n",
    "# Open CSV File for Writing\n",
    "with open(csv_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Write Header\n",
    "    writer.writerow([\"id\", \"script\"])\n",
    "\n",
    "    for idx, text in enumerate(translated_list_second):\n",
    "        writer.writerow([idx, text])\n",
    "    \n",
    "print(f\"Transcription and translation saved to {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index with Germany Text: []\n"
     ]
    }
   ],
   "source": [
    "#rechecking the if any rows did not get translated\n",
    "df_checked = pd.read_csv(\"data/translated_scripts_checked.csv\")\n",
    "\n",
    "df_checked[\"language_checked\"] = df_checked[\"script\"].astype(str).apply(detect_language)\n",
    "\n",
    "german_index = df_checked[df_checked[\"language_checked\"] == \"de\"].index.tolist()\n",
    "\n",
    "print(f\"index with Germany Text: {german_index}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
